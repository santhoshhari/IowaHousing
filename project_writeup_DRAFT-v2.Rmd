---
title: 'Linear Regression Analysis: <br> Regression Case Study'
author: "Neerja Doshi, Sri Santhosh Hari, Ker-Yu Ong, Nicha Ruchirawat"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, tidy.opts=list(width.cutoff=65),tidy=TRUE)

# Clear working environment
rm(list=ls())

# Load requisite packages 
if(!require("tidyverse")){install.packages("tidyverse", repos = "http://cran.us.r-project.org")}
if(!require("glmnet")){install.packages("glmnet", repos = "http://cran.us.r-project.org")}
if(!require("ggplot2")){install.packages("ggplot2", repos = "http://cran.us.r-project.org")}
if(!require("magrittr")){install.packages("magrittr", repos = "http://cran.us.r-project.org")}
if(!require("gridExtra")){install.packages("gridExtra", repos = "http://cran.us.r-project.org")}
if(!require("olsrr")){install.packages("olsrr", repos = "http://cran.us.r-project.org")}
if(!require("data.table")){install.packages("data.table", repos = "http://cran.us.r-project.org")}
if(!require("MASS")){install.packages("MASS", repos = "http://cran.us.r-project.org")}
if(!require("car")){install.packages("car", repos = "http://cran.us.r-project.org")}
if(!require("perturb")){install.packages("perturb", repos = "http://cran.us.r-project.org")}

# Turn off scientific notation
options(scipen=999)

```

# Part I: Explanatory Modelling

## Task 0: Exploratory Data Analysis and Data Cleaning

```{r}
#rawDF <- read.csv("/Users/booranium/usf/601_regression/project/housing.txt", stringsAsFactors = T)
rawDF <- read.csv("/Users/santhoshhari/Documents/Coursework/LinearRegression/IowaHousing/Data/housing.txt", stringsAsFactors = T)
#rawDF <- read.csv('housing.txt', stringsAsFactors = T)

```

The Iowa housing dataset contains `r dim(rawDF)[1]` rows and `r dim(rawDF)[2]` variables, a glimpse of which is as follows: 

```{r}
str(rawDF)
```

At first glance, we see that most of the variables are categorical - both numeric and character types - and only a handful are continuous. The response variable for our analysis is `SalePrice`, and the remaining 79 variables (excluding the record `ID` column) are considered potential predictor variables. Checking the data dictionary, we found the following distribution for the predictor variables:

* 49 categorical
* 19 are continuous, e.g. area, price 
* 11 are discrete, e.g. count, year 

There are `r nrow(rawDF) - nrow(unique(rawDF))` duplicate rows in the dataset.

#### Handling NA Values

Below, we compute that number and percentage of `NA`s per variable in the dataset having at least 1 `NA`.

```{r}
NA_columns <- colnames(rawDF)[unique(which(is.na(rawDF), arr.ind = T)[,2])]

NA_count <- rawDF %>%
            dplyr::select(NA_columns) %>%
            summarise_all(funs(sum(is.na(.)))) %>%
            gather(key = "Variable", value = "num_na", everything()) %>%
            arrange(desc(num_na))

NA_count %<>% mutate(perc_na = paste(round(num_na/nrow(rawDF),4)*100,"%"))
colnames(NA_count) <- c("**Variable**", "**Number of NA**", "**Percentage of NA**")
row.names(NA_count) <- NULL
knitr::kable(NA_count, caption = "\\label{tab:NACount} Variable NA Count and Percentage",
format.args = list(big.mark = ','))
```

The data dictionary tells us that for most of the fields in Table \ref{tab:NACount}, `NA` is actually meaningful, indicating non-applicability or a lack of the feature rather than missing data. After checking the data dictionary for the meaning of each field, we imputed - for every categorical variable for which `NA` was meaningful - `NA`s with `0`s.

```{r}
# Create a copy of rawDF to be our working data frame
housingDF <- rawDF 

# Update NAs with 0s for applicable fields
levels(housingDF$PoolQC) <- c("0",levels(housingDF$PoolQC))
housingDF$PoolQC[is.na(housingDF$PoolQC)] <- "0"
levels(housingDF$MiscFeature) <- c("0", levels(housingDF$MiscFeature))
housingDF$MiscFeature[is.na(housingDF$MiscFeature)] <- "0"
levels(housingDF$Alley) <- c("0", levels(housingDF$Alley))
housingDF$Alley[is.na(housingDF$Alley)] <- "0"
levels(housingDF$Fence) <- c("0", levels(housingDF$Fence))
housingDF$Fence[is.na(housingDF$Fence)] <- "0"
levels(housingDF$FireplaceQu) <- c("0", levels(housingDF$FireplaceQu))
housingDF$FireplaceQu[is.na(housingDF$FireplaceQu)] <- "0"
levels(housingDF$GarageType) <- c("0", levels(housingDF$GarageType))
housingDF$GarageType[is.na(housingDF$GarageType)] <- "0"
levels(housingDF$GarageFinish) <- c("0", levels(housingDF$GarageFinish))
housingDF$GarageFinish[is.na(housingDF$GarageFinish)] <- "0"
levels(housingDF$GarageQual) <- c("0", levels(housingDF$GarageQual))
housingDF$GarageQual[is.na(housingDF$GarageQual)] <- "0"
levels(housingDF$GarageCond) <- c("0", levels(housingDF$GarageCond))
housingDF$GarageCond[is.na(housingDF$GarageCond)] <- "0"
levels(housingDF$BsmtExposure) <- c("0", levels(housingDF$BsmtExposure))
housingDF$BsmtExposure[is.na(housingDF$BsmtExposure)] <- "0"
levels(housingDF$BsmtFinType2) <- c("0", levels(housingDF$BsmtFinType2))
housingDF$BsmtFinType2[is.na(housingDF$BsmtFinType2)] <- "0"
levels(housingDF$BsmtQual) <- c("0", levels(housingDF$BsmtQual))
housingDF$BsmtQual[is.na(housingDF$BsmtQual)] <- "0"
levels(housingDF$BsmtCond) <- c("0", levels(housingDF$BsmtCond))
housingDF$BsmtCond[is.na(housingDF$BsmtCond)] <- "0"
levels(housingDF$BsmtFinType1) <- c("0", levels(housingDF$BsmtFinType1))
housingDF$BsmtFinType1[is.na(housingDF$BsmtFinType1)] <- "0"
```

We then re-check the count and percentage of `NA`s per variable left in the dataset.

```{r}
NA_columns <- colnames(housingDF)[unique(which(is.na(housingDF), arr.ind = T)[,2])]

NA_count <- housingDF %>% 
            dplyr::select(NA_columns) %>%
            summarise_all(funs(sum(is.na(.)))) %>%
            gather(key = "Variable", value = "num_na", everything()) %>%
            arrange(desc(num_na))

NA_count %<>% mutate(perc_na = paste(round(num_na/nrow(housingDF),4)*100,"%"))
colnames(NA_count) <- c("**Variable**", "**Number of NA**", "**Percentage of NA**")
row.names(NA_count) <- NULL
knitr::kable(NA_count, caption = "\\label{tab:NACount1} Variable NA Count and Percentage(after replacing NAs with 0s, where appropriate)",
format.args = list(big.mark = ','))

colnames(housingDF)
```

Table \ref{tab:NACount1} shows the list of remaining variables where `NA` indicates missing data. We impute `NA`s in these variables with

* mean of the data, for continuous variables (LotFrontage)
* median of the data, for discrete variables (GarageYrBlt)
* mode of the data, for categorical variables (MasVnrType, Electrical)

```{r}
# Function to get mode of data
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Impute NAs 
housingDF$LotFrontage[is.na(housingDF$LotFrontage)] <- mean(housingDF$LotFrontage, na.rm = T)
housingDF$GarageYrBlt[is.na(housingDF$GarageYrBlt)] <- median(housingDF$GarageYrBlt, na.rm=T)
housingDF$MasVnrType[is.na(housingDF$MasVnrType)] <- getmode(housingDF$MasVnrType)
housingDF$MasVnrArea[is.na(housingDF$MasVnrArea)] <- 0
housingDF$Electrical[is.na(housingDF$Electrical)] <- getmode(housingDF$Electrical)

# Convert MSSubClass to factor
housingDF$MSSubClass <- factor(housingDF$MSSubClass)
housingDF$MoSold <- factor(housingDF$MoSold)
```

Since Masonry veneer area (`MasVnrArea`) is directly related to `MasVnrType`, we impute for area based on the mode of `MasVnrType`, which is `r getmode(housingDF$MasVnrType)`. Our cleaned dataset is named `housingDF`. 

#### Exploratory Data Visualization

With our clean dataset, we perform exploratory data visualization of the distribution of key measures such as volume and sale price of houses by what we hypothesize to be key predictor variables.

To begin with, we check the distribution of sale prices using a histogram and box-plot.

```{r}
#hist(housingDF$SalePrice, main = "Histogram of Sale Price")
#boxplot(housingDF$SalePricem, main = "Boxplot of Sale Price")
summary(housingDF$SalePrice)
```

Intuition suggests the neighborhood is a key determining factor in a house's sale price, hence below, we plot the distribution of sale price by neighborhood. 

```{r, fig.cap="\\label{fig:box1} SalePrice distribution per neighborhood"}
housingDF %>% 
  dplyr::select(Neighborhood, SalePrice) %>% 
  ggplot(aes(factor(Neighborhood), SalePrice)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust =1)) + 
  xlab('Neighborhoods')
```

From Figure \ref{fig:box1}, we can observe that Brookside and Meadow Vista have the lowest median house price while Northridge and Northridge Height have the heighest median house price as well as several outliers.

We then distribution of houses by a number of key features we hypothesize to be important in determining housing price: the property's zoning class (`MSZoning`), type of road access to the property (`Street`), type of alley access to the property (`Alley`), and type of utilies available (`Utilities`)

```{r}
plotHist <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=factor(x))) +
       stat_count() +
       xlab(colnames(data_in)[i]) +
       theme_light() + 
       theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}

doPlots <- function(data_in, fun, ii, ncol=3) {
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}

plotDen <- function(data_in, i){
  data <- data.frame(x=data_in[[i]], SalePrice = data_in$SalePrice)
  p <- ggplot(data= data) + 
       geom_line(aes(x = x), stat = 'density', size = 1,alpha = 1.0) +
       xlab(paste0((colnames(data_in)[i]), '\n', 'Skewness: ',
                   round(skewness(data_in[[i]], na.rm = TRUE), 2))) +
       theme_light() 
  return(p)
}

plotCorr <- function(data_in, i){
  data <- data.frame(x = data_in[[i]], SalePrice = data_in$SalePrice)
  p <- ggplot(data, aes(x = x, y = SalePrice)) + 
       geom_point(na.rm = TRUE) + 
       geom_smooth(method = lm ) + 
       xlab(paste0(colnames(data_in)[i], '\n', 'R-Squared: ',
                round(cor(data_in[[i]], data$SalePrice, use = 'complete.obs'), 2))) +
       theme_light()
  return(suppressWarnings(p))
}
```

```{r, fig.height=3.5, fig.cap="\\label{fig:hist1}Locality, access, utility features distribution"}
doPlots(housingDF, fun = plotHist, ii = c(3,6,7,10), ncol = 2)
```

We also plot the distribution of houses against a number of features related to the physical geography of the property:

```{r, fig.height=3.5, fig.cap="\\label{fig:hist2}Lot/Land feature distribution"}
doPlots(housingDF, fun = plotHist, ii = c(8,9,11,12), ncol = 2)
```

Figure \ref{fig:hist1} suggests that most of the houses are located in `Medium/Low Density` residential areas. We can also observe that most of the houses have paved road access, do not have alleys and have all public utilities(E,G,W,& S). From Figure \ref:{fig:hist2}, we can notice that most of the properties are regular or slightly irregular in share, built on level surfaces with gentle slope. 

```{r, fig.height=3.5, fig.cap="\\label{fig:hist3} Neighborhood level slope distribution"}
housingDF %>% 
  dplyr::select(LandSlope, Neighborhood) %>% 
  arrange(Neighborhood) %>% 
  group_by(Neighborhood, LandSlope) %>% 
  summarize(Count = n()) %>% 
  ggplot(aes(Neighborhood, Count)) + 
  geom_bar(aes(fill = LandSlope), position = 'dodge', stat = 'identity')+
  theme(axis.text.x = element_text(angle = 90, hjust =1))
```

From Figure \ref:{fig:hist3}, we can see that houses with severe slope are located only in Clear Creek and Timberland while more than 10 neighborhoods have properties with moderate slope.

```{r, fig.height=8, fig.cap="\\label{fig:hist3} Scatter plot of variables showing high positive  linear relationship with SalePrice"}
num_var <- names(housingDF)[which(sapply(housingDF, is.numeric))]
housing_numeric <- housingDF[num_var]
correlations <- cor(housing_numeric[,-1])
highcorr <- c(names(correlations[,'SalePrice'])[which(correlations[,'SalePrice'] > 0.5)], names(correlations[,'SalePrice'])[which(correlations[,'SalePrice'] < -0.2)])
data_corr <- housingDF[highcorr]
doPlots(data_corr, fun = plotCorr, ii = 1:10)
```

## Task 1. Building the Explanatory Model

####  Testing for Influential Points 

Having dealt with the `NA`s in our dataset, we use the `model.matrix()` function from the `glmnet` package to convert each categorical variable into an appropriate set of binary indicators: for a categorical variable that takes k levels, `model.matrix()` produces k-1 binary indicators. We then reappend our response vector `SalePrice` to the resulting wide design matrix `designDF` to create `workingDF`, which includes both the converted predictors and response variables.

```{r}
designDF <- model.matrix(SalePrice ~ ., data = housingDF)[,-1]
designDF <- as.data.frame(designDF)
workingDF <- cbind(designDF, SalePrice = housingDF$SalePrice)
```

In looking for influential points, we leverage the `OLSRR` package to test observations for influence according to the DFFITS diagnostic. We do this by first fitting a saturated model on `workingDF` and then calling `ols_dffits_plot()` on it.  

```{r}
ols_model <- lm(SalePrice ~ ., data = workingDF)
ols_dffits_plot(ols_model)

# identify threshold t for points of influence
n = nrow(workingDF)
p = ncol(workingDF - 1) # remove response var 
t = 2*sqrt(p/n)
```

Note that according to the criterion of threshold t = 2*sqrt(n/p) = 0.85, the DFFITS plot shows a large number of influential observations. We look specifically at 6 points greater than threshold = abs(5): 198, 524, 692, 826 and 1183. Below we plot the standardized and studentized residuals to check these observations for being outliers and/or points of leverage respectively. 

```{r}
# Creating from scratch because OLSRR ols_srsd_plot() function is not working.

# Create df of studentized residuals 
student_resid <- as.data.frame(rstudent(ols_model))
student_resid <- setDT(student_resid, keep.rownames = TRUE)[]
colnames(student_resid) <- c('ix', 'resid')

# Plot
plot(student_resid$ix, student_resid$resid, col = ifelse(workingDF$Id == 198 | workingDF$Id == 524 | workingDF$Id == 692 | workingDF$Id == 826 | workingDF$Id == 1183, "red", "black"), pch = ifelse(workingDF$Id == 198 | workingDF$Id == 524 | workingDF$Id == 692 | workingDF$Id == 826 | workingDF$Id == 1183, 19, 1), main = "Plot of Studentized Residuals", xlab = 'Observation' )
abline(h = t, col = 'red')
abline(h = 0, col = 'red')
abline(h = -t, col = 'red')
text(student_resid$ix, student_resid$resid, labels = student_resid$ix)
```
Our plot of studentized resdiuals indicates that observations all 5 points are leverage points.

```{r}
# Create df of standardized residuals 
standard_resid <- as.data.frame(rstandard(ols_model))
standard_resid <- setDT(standard_resid, keep.rownames = TRUE)[]
colnames(standard_resid) <- c('ix', 'resid')

# Plot
plot(standard_resid$ix, standard_resid$resid, col = ifelse(workingDF$Id == 198 | workingDF$Id == 524 | workingDF$Id == 692 | workingDF$Id == 826 | workingDF$Id == 1183, "red", "black"), pch = ifelse(workingDF$Id == 198 | workingDF$Id == 524 | workingDF$Id == 692 | workingDF$Id == 826 | workingDF$Id == 1183, 19, 1), main = "Plot of Standardized Residuals", xlab = 'Observation' )
abline(h = t, col = 'red')
abline(h = 0, col = 'red')
abline(h = -t, col = 'red')
text(standard_resid$ix, standard_resid$resid, labels = standard_resid$ix)
```

Our plots of studentized and standardized resdiuals indicate that all 5 observations are both points of leverage and outliers. We remove them from our dataset and recreate the saturated OLS model below:

```{r}
# remove influential points 
workingDF <- filter(workingDF, !Id %in% c(198, 524, 692, 826, 1183)) 
#nrow(workingDF) #1455

# recreate model
ols_model <- lm(SalePrice ~ ., data = workingDF)
```

For the purposes of variable selection, we refer to the saturated OLS model created above and perform stepwise model selection according to both AIC and BIC criterions.

```{r, eval = FALSE}
# Code chunk not run; load saved model for expediency 
model_aic <- step(ols_model, direction = 'backward', trace = F) 
model_bic <- step(ols_model, k = log(nrow(workingDF)), direction = 'backward', trace = F) 

## save the models 
save(model_aic, file = "/Users/santhoshhari/Documents/Coursework/LinearRegression/IowaHousing/AIC_model.rda")
save(model_bic, file = "/Users/santhoshhari/Documents/Coursework/LinearRegression/IowaHousing/BIC_model.rda")
```

Per the AIC criterion, the following are the predictor variables signficant at the alpha = 0.05 level.

```{r}
#load("/Users/booranium/usf/601_regression/project/IowaHousing/AIC_model.rda") # model loaded as 'model_aic' 
#load("/Users/santhoshhari/Documents/Coursework/LinearRegression/IowaHousing/AIC_model.rda") # model loaded as 'model_aic' 
load('AIC_model.rda')

# Find coefficients significant at the alpha = 0.01 level
bool_aic <- summary(model_aic)$coeff[-1,4] < 0.01
sig_var_aic <- names(bool_aic)[bool_aic == TRUE]
```

Per the BIC criterion, the following are the predictor variables signficant at the alpha = 0.05 level.

```{r}
#load("/Users/booranium/usf/601_regression/project/IowaHousing/BIC_model.rda") # model loaded as 'model_bic'
#load("/Users/santhoshhari/Documents/Coursework/LinearRegression/IowaHousing/BIC_model.rda") # model loaded as 'model_bic' 
load('BIC_model.rda')
# find coefficients significant at the alpha = 0.01 level
bool_bic <- summary(model_bic)$coeff[-1,4] < 0.01
sig_var_bic <- names(bool_bic)[bool_bic == TRUE]
```

Note that both criterions select the same set of variables.
```{r}
setdiff(sig_var_aic, sig_var_bic)
```

We can now perform OLS regression with our subset of `r length(sig_var_bic)` significant variables. The model summary is as follows:

```{r}
model_sig_formula <- as.formula(paste("SalePrice ~ ", paste(sig_var_bic, collapse = "+")))
model_sig <- lm(formula = model_sig_formula, data = workingDF)

# create a model with scaled Xs and Y for multicollinearity detection 
model_sig_scaled <- lm(formula = model_sig_formula, data = as.data.frame(scale(workingDF)))
```

We check for multicollinearity in our model by checking for Variance Inflation Factors:

```{r}
vif(model_sig_scaled)[(sqrt(vif(model_sig_scaled)) > 10) == TRUE]
```
We see that there are two variables with VIF values > threshold = 10. This tells us there is multicollinearity present in the dataset. We verify this by checking the Singular Value Criteria for multicollinearity: 

```{r}
coll_out = colldiag(model_sig, scale = TRUE, center = FALSE, add.intercept = TRUE)
coll_out$condindx[(coll_out$condindx > 30)== TRUE]
```
We see that there are several entries > threshold = 30, and hence we conclude that multicollinearity exists. In order to identify which variables are multicollinear:

```{r}
coll_out = colldiag(model_sig, scale = TRUE, center = FALSE, add.intercept = TRUE)
# unlist(coll_out[1], use.names = F) >30
# colnames(coll_out$pi)[unlist(coll_out[1], use.names = F) >30]
# dim(as.matrix(coll_out$pi[unlist(coll_out[1], use.names = F) >30, ]))
```

Based on results, we drop the Garage Condition variables since they are collinear with the Garage Quality variables.

```{r}
# Drop GarageCondition variables since they are correlated with Garage Quality variables 
drop = c('GarageCondEx','GarageCondFa', 'GarageCondGd', 'GarageCondPo' )
new_sig_var_bic = sig_var_bic [! sig_var_bic %in% drop]
```

We then rerun the model and recheck for multicollinearity using VIFS:

```{r}
new_model_formula <- as.formula(paste("SalePrice ~ ", paste(new_sig_var_bic, collapse = "+")))
new_model <- lm(formula = new_model_formula, data = workingDF)

# create a model with scaled Xs and Y for multicollinearity detection 
new_model_scaled <- lm(formula = new_model_formula, data = as.data.frame(scale(workingDF)))

# check for MC
vif(new_model_scaled)[(sqrt(vif(new_model_scaled)) > 10) == TRUE]
```

Using our rule of thumb, we conclude that there is no more multicollinearity in our model since there are no VIF values > 10. We print the summary of our model below:

```{r}
summary(new_model)
```


At first glance, the multiple R-squared value of 0.9293 indicates that 92.93% of the variability in SalePrice around its mean is explained by the mode, i.e. by the predictor variables that have been included. This suggests a high-performing explanatory model. 

Let Before welcoming this conclusion, we validate the linearity and normality assumptions of our model by checking our residuals as follows: 

```{r}
# Residual plots 

res = resid(new_model) # residuals 
stdres = rstandard(new_model) # standardized residuals 

# QQ Plot of Residuals - for normality
qqnorm(stdres, main = "QQ Plot of Standardized Residuals", xlab = "Normal Scores", ylab = "Standardized Residual")
qqline(stdres)

# Shapiro-Wilks test for normality 
shapiro.test(res)

# Plot of residuals vs. fitted values - for linearity 
plot(workingDF$SalePrice, res, main = "Plot of Residuals vs. Sale Price", xlab = "Sale Price", ylab = "Residual")
#abline(h=c(0,0), col = 'red')
#abline(v=350000, col = 'blue')


```

The plot of residuals against fitted values shows that for the most part, the residuals are evenly distributed across the y = 0 line. However, we see that as Sale Price increases, the residuals start to deviate homoscedasticity. More specifically, we see this deviation happen at approximately Sale Price = $350K, which our earlier summary showed to be between the variable's 3rd quartile and maximum. This suggets that for the last quartile of high-priced houses, the fitted regression model is not as adequate as it is for the reset of the population. The normal probability (QQ) plot corroborates this finding: it shows deviance from linearity at both tail ends of the residual rang, which suggests a heavy tailed distribution. This occurs at both ends of the distribution, i.e. both extremely low-priced houses and extremely high-priced houses are pulling the distribution away from normality. Indeed, the formal Shapiro-Wilks test for normality produces a p-value of ~0, which leads us to reject the null hypothesis, at the alpha = 0.05 level, that the residuals are normally distributed. 

As a remedial measure, we consider performing a transformation on Sale Price. We use the `boxcox()` function to determine the transformation under which the maximum likelihood [of ?] is attained. 

```{r}
boxcox(new_model)
```

We see that lambda = 1 is not captured in the 95% CI of lambdas, indicated by the three vertical dashed lines. This means a transformation is necessary. We choose lambda = ~ 0.5 since this is an interpretable transformation value. Below, we apply a square root transformation to Sale Price, refit the model, and revalidate our model assumptions with residual plots as above.  

```{r}
# Log Transformation
log_model_formula <- as.formula(paste("sqrt(SalePrice) ~ ", paste(new_sig_var_bic, collapse = "+")))
log_model <- lm(formula = log_model_formula, data = workingDF)

res_log = resid(log_model) # residuals 
stdres_log = rstandard(log_model) # standardized residuals 

shapiro.test(res_log)

# Plot of Residuals from Log Model vs. Fitted Values 
plot(log(workingDF$SalePrice), res_log, main = "Plot of Log Model Residuals vs. Log of Sale Price", xlab = "Log of Sale Price", ylab = "Residual")
abline(h=c(0,0), col = 'red')
abline(v=350000, col = 'blue')

# Normal Probability Plot of Residuals from Log Model
qqnorm(stdres_log, main = "QQ Plot of Standardized Log Model Residuals", xlab = "Normal Scores", ylab = "Standardized Residual")
qqline(stdres_log)

```

Our new model produces an R-squared value of 0.9411, indicating that 94.11% of the variance in Sale Price is captured by the model. Our residuals vs. fitted values plot shows a better pattern of homoscedasticity, which suggests that our linear regression model adequately captures the trend in the log-transformed data. The normal probability plot of the standardized residuals also shows better adherence to a linear pattern, which suggests that our assumption of normality is better. 

Accepting our new model, we provide its summary as follows:

```{r}
summary(log_model)
```

We conclude that the variables included above are most relevant in determining a house's sale price. In particular, those variables that are significant at the 0.01 level, i.e., are most significant. 

## Task 2: Making recommendations

```{r}
#morty <- read.csv("/Users/booranium/usf/601_regression/project/Morty.txt", stringsAsFactors = T)
morty <- read.csv("/Users/santhoshhari/Documents/Coursework/LinearRegression/IowaHousing/Data/Morty.txt", stringsAsFactors = T)
morty <- morty[,-1]
new_data <- rbind(housingDF, morty)
new_data <- model.matrix(SalePrice ~ ., data = new_data)[,-1]
x_morty <- as.data.frame(tail(new_data,1))
new_sig_var_bic[which(new_sig_var_bic=="`RoofMatlTar&Grv`")] <- "RoofMatlTar&Grv"
new_sig_var_bic[which(new_sig_var_bic=="`Exterior1stWd Sdng`")] <- "Exterior1stWd Sdng"
new_sig_var_bic[which(new_sig_var_bic=="`Exterior2ndWd Sdng`")] <- "Exterior2ndWd Sdng"
sig_var_morty <- x_morty[,new_sig_var_bic]
```

The estimated sale price of Morty's property is \$ $`r round(predict(log_model,sig_var_morty)^2,0)`$. To make a recommendation to Morty regarding the selling price of his house, we leverage the model built above to make a prediction of Sale Price based on select attributes as determined above.

We see that the 95% CI for the predicted Sale Price of Morty's house, based on selected attributes, is ... As such, we recommend that Morty sell his house at a maximum of ..., which is more/less than the other firm's recommendation of $143K.

```{r}
coef <- data.frame(coef.name = names(coef(log_model)), 
           coef.value = matrix(coef(log_model)))

# exclude the (Intercept) term
coef <- coef[-1,]
coef <- arrange(coef,-coef.value)
imp_coef <- head(coef,20)
ggplot(imp_coef) +
    geom_bar(aes(x=reorder(coef.name,coef.value),y=coef.value), stat="identity") +
    coord_flip() +
    ggtitle("Coefficents in the Model") +
    theme(axis.title=element_blank())
```


# Part II: Predictive Modelling

For comparing the prediction accuracy, we use 4 models - 
1. OLS on log transformed data
2. Ridge Regression
3. Lasso Regression
4. Elastic Net

Transformations done on the data before building models - 
1. Imputing NAs
2. Creating dummy variables
3. Removing influential points

Train and test sets:
75% of the data forms the train set
25% of the data forms the test set



Log Model:
We first normalised y by taking log and then fit the linear model lm.

```{r}
x <- workingDF[,-1]
x$SalePrice <- log(workingDF[,ncol(workingDF)])
#y_log = log(y)
#train/test
set.seed(121)
train <- sample(1:nrow(x), 3 * nrow(x)/4)
test <- (-train)

new_data <- model.matrix(SalePrice ~ ., data = x)
colnames(new_data)[which(colnames(new_data)=="`RoofMatlTar&Grv`")] <- "RoofMatlTar&Grv"
colnames(new_data)[which(colnames(new_data)=="`Exterior1stWd Sdng`")] <- "Exterior1stWd Sdng"
colnames(new_data)[which(colnames(new_data)=="`Exterior2ndWd Sdng`")] <- "Exterior2ndWd Sdng"
new_sig_var_bic[which(new_sig_var_bic=="`RoofMatlTar&Grv`")] <- "RoofMatlTar&Grv"
new_sig_var_bic[which(new_sig_var_bic=="`Exterior1stWd Sdng`")] <- "Exterior1stWd Sdng"
new_sig_var_bic[which(new_sig_var_bic=="`Exterior2ndWd Sdng`")] <- "Exterior2ndWd Sdng"
filtered_data <- new_data[,new_sig_var_bic]
filtered_data <- cbind(filtered_data, x$SalePrice)
colnames(filtered_data)[length(colnames(filtered_data)) ] = 'SalePrice'
# Build model
#log_model <- lm(SalePrice~., data = data.frame(filtered_data[train,]))

# Validation set:
y_train <- filtered_data[test, 1:ncol(filtered_data)-1]
y_test <- filtered_data[test, ncol(filtered_data)]
predictions_log <- predict(log_model, as.data.frame(y_train))
(mean(((predictions_log)^2 - (y_test))^2))

```

Ridge:
```{r}
set.seed(121)

x <- workingDF[,2:ncol(workingDF)-1]
y <- workingDF$SalePrice

#train/test
y.train <- y[train]
y.test <- y[test]

ridge<-cv.glmnet(as.matrix(x[train, ]), y.train, alpha=0)
plot(ridge)
best.lambda <- ridge$lambda.min
best.lambda
abline(v = log(best.lambda), col = 'blue', lwd = 2)

ridge.model.train <- glmnet(as.matrix(x[train, ]), y.train, alpha = 0, lambda = best.lambda)


ridge.pred <- predict(ridge.model.train, s = best.lambda, newx = as.matrix(x[test,]))
mspe.ridge <- mean((ridge.pred - y.test)^2)

coef_ridge <- coef(ridge.model.train)
length(coef_ridge[coef_ridge != 0])

```

Lasso:
```{r}

set.seed(121)

x <- workingDF[,2:ncol(workingDF)-1]
y <- workingDF$SalePrice

lasso <-cv.glmnet(as.matrix(x[train, ]), y.train, alpha=1)
plot(lasso)
best.lambda <- lasso$lambda.min
best.lambda
abline(v = log(best.lambda), col = 'blue', lwd = 2)

lasso.model.train <- glmnet(as.matrix(x[train, ]), y.train, alpha = 1, lambda = best.lambda)


lasso.pred <- predict(lasso.model.train, s = best.lambda, newx = as.matrix(x[test,]))
mspe.lasso <- mean((lasso.pred - y.test)^2)
mspe.lasso
#coef(lasso)
coef_lasso <- coef(lasso.model.train)
length(coef_lasso[coef_lasso!= 0])



```

Elastic Net:

```{r}

set.seed(121)
x <- workingDF[,2:ncol(workingDF)-1]
y <- workingDF$SalePrice
#find best alpha
alphalist<-seq(0,1,by=0.1)
elasticnet<-lapply(alphalist, function(a){cv.glmnet(as.matrix(x[train, ]), y.train, alpha=a)})
mse <- list()
for (i in 1:11) {
  mse <- c(mse,min(elasticnet[[i]]$cvm) )
  print(min(elasticnet[[i]]$cvm)) 
  print(i)}
which.min(unlist(mse))
alpha_min = alphalist[which.min(unlist(mse))]

#find best lambda for best alpha
cv.out <- cv.glmnet(as.matrix(x[train, ]), y.train, alpha = alpha_min)

plot(cv.out)
best.lambda <- cv.out$lambda.min
best.lambda
abline(v = log(best.lambda), col = 'blue', lwd = 2)

#train EN
EN.model.train <- glmnet(as.matrix(x[train, ]), y.train, alpha = alpha_min, lambda = best.lambda)

EN.pred <- predict(EN.model.train, newx = as.matrix(x[test,]))
mspe.EN <- mean((EN.pred - y.test)^2)
coef_elastic <- coef(EN.model.train)
length(coef_elastic[coef_elastic!= 0])
```

```{r}
MSPE <- data.frame(Ridge = mspe.ridge, Lasso = mspe.lasso, EN = mspe.EN)
```

